"""
RAG Engine Module
Retrieval-Augmented Generation sistemi ve kiÅŸiselleÅŸtirme.
"""

from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_ollama import ChatOllama
from langchain_core.prompts import ChatPromptTemplate
import os

# VektÃ¶r veritabanÄ± kaydetme/yÃ¼kleme yolu
VECTORSTORE_PATH = "data/vectorstore"

def create_vector_db(text, persist=False):
    """
    Metni vektÃ¶rlere Ã§evirir.
    
    Args:
        text: VektÃ¶rleÅŸtirilecek metin
        persist: VektÃ¶r veritabanÄ±nÄ± diske kaydet
    
    Returns:
        FAISS vectorstore
    """
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=750, 
        chunk_overlap=150,
        separators=["\n\n", "\n", ". ", " ", ""]
    )
    chunks = text_splitter.split_text(text)
    
    # Ã‡ok dilli embedding modeli - TÃ¼rkÃ§e iÃ§in optimize edilmiÅŸ
    embeddings = HuggingFaceEmbeddings(
        model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
        model_kwargs={'device': 'cpu'}
    )
    
    vectorstore = FAISS.from_texts(texts=chunks, embedding=embeddings)
    
    # KalÄ±cÄ± kayÄ±t
    if persist:
        os.makedirs(VECTORSTORE_PATH, exist_ok=True)
        vectorstore.save_local(VECTORSTORE_PATH)
    
    return vectorstore

def load_vector_db():
    """KayÄ±tlÄ± vektÃ¶r veritabanÄ±nÄ± yÃ¼kler."""
    if os.path.exists(VECTORSTORE_PATH):
        embeddings = HuggingFaceEmbeddings(
            model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
            model_kwargs={'device': 'cpu'}
        )
        return FAISS.load_local(VECTORSTORE_PATH, embeddings, allow_dangerous_deserialization=True)
    return None

def add_to_vector_db(text, existing_vectorstore=None):
    """Mevcut vektÃ¶r veritabanÄ±na yeni metin ekler."""
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=750, 
        chunk_overlap=150
    )
    chunks = text_splitter.split_text(text)
    
    embeddings = HuggingFaceEmbeddings(
        model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
        model_kwargs={'device': 'cpu'}
    )
    
    if existing_vectorstore:
        # Mevcut veritabanÄ±na ekle
        existing_vectorstore.add_texts(chunks)
        return existing_vectorstore
    else:
        # Yeni oluÅŸtur
        return FAISS.from_texts(texts=chunks, embedding=embeddings)

def get_personalized_context(user_id: int = None):
    """KiÅŸiselleÅŸtirme iÃ§in kullanÄ±cÄ± baÄŸlamÄ± oluÅŸturur.
    
    Args:
        user_id: KullanÄ±cÄ± ID (multi-tenant iÃ§in zorunlu)
    
    Returns:
        (user_profile, learning_context) tuple
    """
    if not user_id:
        return "KullanÄ±cÄ± hakkÄ±nda Ã¶zel bilgi yok.", ""
    
    try:
        from .memory_engine import build_memory_context
        memory_context = build_memory_context(user_id)
        
        if memory_context:
            return memory_context, ""
        else:
            return "KullanÄ±cÄ± hakkÄ±nda Ã¶zel bilgi yok.", ""
    except Exception as e:
        print(f"Memory context error: {e}")
        return "KullanÄ±cÄ± hakkÄ±nda Ã¶zel bilgi yok.", ""

def get_ai_response(model_name, vectorstore, user_question, chat_history=None, user_id=None):
    """
    Ollama'ya soruyu sorar. KiÅŸiselleÅŸtirilmiÅŸ yanÄ±t dÃ¶ndÃ¼rÃ¼r.
    
    Args:
        model_name: KullanÄ±lacak model (llama3, phi3, mistral vb.)
        vectorstore: FAISS vektÃ¶r veritabanÄ±
        user_question: KullanÄ±cÄ±nÄ±n sorusu
        chat_history: Ã–nceki sohbet geÃ§miÅŸi (opsiyonel)
        user_id: KullanÄ±cÄ± ID (kiÅŸiselleÅŸtirme iÃ§in)
    
    Returns:
        tuple: (AI yanÄ±tÄ±, kaynak dokÃ¼manlar)
    """
    try:
        # 1. KiÅŸiselleÅŸtirme bilgilerini al (user_id ile)
        user_profile, learning_context = get_personalized_context(user_id=user_id)

        # 2. Benzer iÃ§erikleri bul
        docs = vectorstore.similarity_search(user_question, k=4)
        pdf_context = "\n\n".join([doc.page_content for doc in docs])
        
        # 3. Sohbet geÃ§miÅŸini hazÄ±rla
        history_text = ""
        if chat_history:
            recent_history = chat_history[-6:]  # Son 3 soru-cevap
            for msg in recent_history:
                role = "KullanÄ±cÄ±" if msg["role"] == "user" else "Asistan"
                history_text += f"{role}: {msg['content'][:200]}\n"
        
        # 4. GeliÅŸmiÅŸ prompt - Chain of Thought + TÃ¼rkÃ§e yanÄ±t
        template = """Sen LocalInsights asistanÄ±sÄ±n - akÄ±llÄ±, yardÄ±msever ve kiÅŸiselleÅŸtirilmiÅŸ bir eÄŸitim asistanÄ±sÄ±n.

âš ï¸ DÄ°L KURALI: SADECE TÃœRKÃ‡E YANIÅ VER. ASLA BAÅKA DÄ°L KULLANMA. NO CHINESE. NO ENGLISH.

KULLANICI BÄ°LGÄ°LERÄ°:
{user_profile}

{learning_context}

DÃ–KÃœMAN Ä°Ã‡ERÄ°ÄÄ°:
{pdf_context}

{history_section}

KULLANICI SORUSU: {question}

DÃœÅÃœNCE SÃœRECÄ° (AdÄ±m adÄ±m dÃ¼ÅŸÃ¼n):
1. Ã–nce kullanÄ±cÄ±nÄ±n ne sorduÄŸunu anla.
2. DÃ¶kÃ¼man iÃ§eriÄŸinde ilgili bilgileri bul.
3. Bilgiyi kullanÄ±cÄ±nÄ±n seviyesine uygun ÅŸekilde aÃ§Ä±kla.
4. Emin olmadÄ±ÄŸÄ±n bilgileri "Bu konuda dokÃ¼manda bilgi bulamadÄ±m" diye belirt.

KRÄ°TÄ°K KURALLAR:
- âš ï¸ SADECE TÃœRKÃ‡E YANIT VER. Ã‡Ä°NCE, Ä°NGÄ°LÄ°ZCE VEYA BAÅKA DÄ°L KULLANMA!
- SADECE DÃ–KÃœMAN Ä°Ã‡ERÄ°ÄÄ°NDEKÄ° bilgileri kullan. Uydurma yapma.
- Bilgi dokÃ¼manda yoksa aÃ§Ä±kÃ§a belirt.
- YapÄ±landÄ±rÄ±lmÄ±ÅŸ ve anlaÅŸÄ±lÄ±r yanÄ±tlar ver.
- KullanÄ±cÄ±ya ismiyle hitap et (KULLANICI BÄ°LGÄ°LERÄ°'nden).

YANIT FORMAT:
- KÄ±sa ve Ã¶z cevaplar ver.
- Gerekirse madde iÅŸaretleri kullan.
- Teknik terimleri aÃ§Ä±kla.

ğŸ‡¹ğŸ‡· TÃœRKÃ‡E YANITINI VER (BAÅKA DÄ°L YASAK):"""
        
        history_section = f"SON SOHBET GEÃ‡MÄ°ÅÄ°:\n{history_text}" if history_text else ""
        
        prompt = ChatPromptTemplate.from_template(template)
        llm = ChatOllama(model=model_name, temperature=0.1)
        chain = prompt | llm
        
        response = chain.invoke({
            "user_profile": user_profile,
            "learning_context": learning_context,
            "pdf_context": pdf_context,
            "history_section": history_section,
            "question": user_question
        })
        
        return response.content, docs
        
    except Exception as e:
        return f"HATA: {e}", []

def get_quick_answer(model_name, question, user_id=None):
    """
    DokÃ¼man olmadan hÄ±zlÄ± cevap verir.
    
    Args:
        model_name: KullanÄ±lacak model
        question: KullanÄ±cÄ±nÄ±n sorusu
        user_id: KullanÄ±cÄ± ID (kiÅŸiselleÅŸtirme iÃ§in)
    
    Returns:
        str: AI yanÄ±tÄ±
    """
    try:
        user_profile, _ = get_personalized_context(user_id=user_id)
        template = """Sen LocalInsights asistanÄ±sÄ±n - akÄ±llÄ± ve yardÄ±msever bir eÄŸitim asistanÄ±.

âš ï¸ DÄ°L KURALI: SADECE TÃœRKÃ‡E YANIT VER. Ã‡Ä°NCE, Ä°NGÄ°LÄ°ZCE VEYA BAÅKA DÄ°L ASLA KULLANMA!

KULLANICI BÄ°LGÄ°LERÄ°: {user_profile}

KULLANICI SORUSU: {question}

DÃœÅÃœNCE SÃœRECÄ°:
1. Soruyu anla.
2. BildiÄŸin bilgilerle kÄ±sa ve net yanÄ±t ver.
3. Emin deÄŸilsen belirt.

KRÄ°TÄ°K KURALLAR:
- âš ï¸ SADECE TÃœRKÃ‡E YANIT VER. NO CHINESE!
- KullanÄ±cÄ±ya ismiyle hitap et.
- KÄ±sa ve samimi ol.
- Uydurma yapma, bilmiyorsan sÃ¶yle.

ğŸ‡¹ğŸ‡· TÃœRKÃ‡E YANITINI VER:"""
        
        prompt = ChatPromptTemplate.from_template(template)
        llm = ChatOllama(model=model_name, temperature=0.2)
        chain = prompt | llm
        
        response = chain.invoke({
            "user_profile": user_profile,
            "question": question
        })
        
        return response.content
        
    except Exception as e:
        return f"HATA: {e}"